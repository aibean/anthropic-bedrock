# Completion

The `Completion` endpoint allows you to give a prompt to an Anthropic LLM and get a respnose.

```python
anthropic.Completion.create(
  model="gpt-3.5-turbo-instruct",
  prompt="Why is the sky blue?"
  max_tokens_to_sample=300
)
```

## Usage

```python
import os
from bedrock_anthropic import AnthropicBedrock

anthropic = AnthropicBedrock(
    access_key=os.getenv("AWS_ACCESS_KEY"),
    secret_key=os.getenv("AWS_SECRET_KEY")
)

completion = anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300
)

print(completion.completion)
```

## Configuration

### model

The model that will complete your prompt.

```python {2}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300
)
```

### prompt

The prompt you want to use.

```python {3}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300
)
```

### max_tokens_to_sample

The maximum number of tokens to generate before stopping.

```python {4}
anthropc.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300
)
```

### stop_sequences (optional)

Sequences that will cause the model to stop generating completion text.

```python {5-7}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300,
    stop_sequences=[
        "sequence"
    ]
)
```

### temperature (optional)

Amount of randomness injected in the response. 0-1

_Note: you can either use temperature, top p, or top k._

```python {5}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300,
    temperature=0.7
)
```

### top_p (optional)

Use nucleus sampling. 0-1

_Note: you can either use temperature, top p, or top k._

```python {5}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300,
    top_p=0.7
)
```

### top_k (optional)

Only sample from the top K options for each subsequent token. 0-1

_Note: you can either use temperature, top p, or top k._

```python {5}
anthropic.Completion.create(
    model="anthropic.claude-v2",
    prompt="Why is the sky blue?",
    max_tokens_to_sample=300,
    top_k=0.7
)
```
