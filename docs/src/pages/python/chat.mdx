# Chat

The `ChatCompletion` endpoint allows you to have a chat conversation with a model.

```python
chat = anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    max_tokens_to_sample=300
)
```

## Usage

```python
import os
from bedrock_anthropic import AnthropicBedrock

anthropic = AnthropicBedrock(
    access_key=os.getenv("AWS_ACCESS_KEY"),
    secret_key=os.getenv("AWS_SECRET_KEY")
)

chat = anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)

print(chat.messages)
```

## Configuration

### model

The model that will complete your prompt.

```python {2}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
```

### messages

The messages to chat with the model

```python {4-7}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
```

### max_tokens_to_sample

The maximum number of tokens to generate before stopping.

```python {3}

anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ]
)
```

### stop_sequences (optional)

Sequences that will cause the model to stop generating completion text.

```python {8-10}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    stop_sequences=[
        "sequence"
    ]
)
```

### temperature (optional)

Amount of randomness injected in the response. 0-1

_Note: you can either use temperature, top p, or top k._

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7
)
```

### top_p (optional)

Use nucleus sampling. 0-1

_Note: you can either use temperature, top p, or top k._

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    top_p=0.7
)
```

### top_k (optional)

Only sample from the top K options for each subsequent token. 0-1

_Note: you can either use temperature, top p, or top k._

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    top_k=0.7
)
```
