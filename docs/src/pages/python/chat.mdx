# Chat

The `ChatCompletion` endpoint allows you to have a chat conversation with a model.

```python
chat = anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    max_tokens_to_sample=300
)
```

## Usage

```python
import os
from bedrock_anthropic import AnthropicBedrock

anthropic = AnthropicBedrock(
    access_key=os.getenv("AWS_ACCESS_KEY"),
    secret_key=os.getenv("AWS_SECRET_KEY")
)

chat = anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)

print(chat["messages"])
```

## Configuration

### model

The model that will complete your prompt. Refer to the models [page](/python/models).

```python {2}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```

### messages

The messages to chat with the model.

```python {4-7}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```

### max_tokens_to_sample (optional)

The maximum number of tokens to generate before stopping.

- Default: `256`
- Range depends on the model, refer to the [models page](/python/models).

```python {3}

anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ]
)
```

### stop_sequences (optional)

Sequences that will cause the model to stop generating completion text.

- Default: `[]`

```python {8-10}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    stop_sequences=[
        "sequence"
    ]
)
```

### temperature (optional)

Amount of randomness injected in the response.

- Default: `1`
- Range: `0-1`

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7
)
```

### top_p (optional)

Use nucleus sampling.

- Default: `1`
- Range: `0-1`

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    top_p=0.7
)
```

### top_k (optional)

Only sample from the top K options for each subsequent token.

- Default: `250`
- Range: `0-500`

```python {8}
anthropic.ChatCompletion.create(
    model="anthropic.claude-v2",
    max_tokens_to_sample=300,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
    ],
    top_k=250
)
```
