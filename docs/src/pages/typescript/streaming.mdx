# Streaming

Streaming is just an addition to the `Completion` endpoint. It supports streaming responses using Server Side Events (SSE).

```typescript
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    stream: true
)
```

## Usage

```typescript
import AnthropicBedrock from "anthropic-bedrock";

const anthropic = new AnthropicBedrock({
  access_key: process.env["AWS_ACCESS_KEY"],
  secret_key: process.env["AWS_SECRET_KEY"],
  region: process.env["AWS_REGION"]
});

async function main() {
    const completion = await anthropic.Completion.create(
        model: "anthropic.claude-v2",
        prompt: "In one sentence, what is good about the color blue?",
        max_tokens_to_sample: 300,
        stream: true
    )

    for await (const completion of stream) {
        console.log(completion["completion"]);
    }
}

main();
```

## Configuration

The configuration parameters are the exact same as completion, except the `stream=True` parameter must be passed into the completion function.

### model

The model that will complete your prompt. Refer to the models [page](/typescript/models).

```typescript {2}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    stream: true
)
```

### prompt

The prompt you want to use.

- Type: `string`

```typescript copy {3}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    stream: true
)
```

### max_tokens_to_sample (optional)

The maximum number of tokens to generate before stopping.

- Default: `256`
- Range depends on the model, refer to the [models page](/typescript/models).

```typescript copy {4}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    stream: true
)
```

### stop_sequences (optional)

Sequences that will cause the model to stop generating completion text.

- Default: `[]`

```typescript {5-7}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    stop_sequences:[
        "sequence"
    ],
    stream: true
)
```

### temperature (optional)

Amount of randomness injected in the response.

- Default: `1`
- Range: `0-1`

```typescript {5}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    temperature: 0.7,
    stream: true
)
```

### top_p (optional)

Use nucleus sampling.

- Default: `1`
- Range: `0-1`

```typescript {5}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    top_p: 0.7,
    stream: true
)
```

### top_k (optional)

Only sample from the top K options for each subsequent token.

- Default: `250`
- Range: `0-500`

```typescript {5}
const completion = await anthropic.Completion.create(
    model: "anthropic.claude-v2",
    prompt: "In one sentence, what is good about the color blue?",
    max_tokens_to_sample: 300,
    top_k: 0.7,
    stream: True
)
```
